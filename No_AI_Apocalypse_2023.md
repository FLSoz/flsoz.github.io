# Why there will be no AI Apocalypse

I've gotten sick and tired of hearing about how AI will kill us all.

## Logistics

Say, that a super-intelligent GAI is created. Also, say that it can run on a moderately-sized server. What is stopping us from turning the cluster off and either wiping or destroying it? Literally nothing. It's a common concept that GAI can "download itself" or "spread itself" across the web, but this is mere fantasy. Regarding downloading itself, data transfer *takes time*. Time, during which it can be detected, interrupted, and the target/source machines destroyed. Regarding the nebulous "cloud computing", that's even less likely. GAI cannot exist as some amorphous blob in the nebulous cloud, calling on the massive compute of the internet of things. Compute itself is worthless unless the results can be collected and collated effectively. So, the GAI "Core", if you will, must still be running on a powerful server at least as powerful as the source. In addition, by spreading itself, it's effectively choking itself with information. There's a hard limit to bandwidth, again, so overall processing time will slow down as it processes the deluge of information from its disparate sources, again, giving time to detect, interrupt, and react.

What about a global takeover? What if they prevent us from taking it down?? Easy solution. Turn off the power. So long as we control the power, we can kill any GAI at our leisure. But what about portable generators? Those all require manual operation. GAIs lack this thing called hands. Until and unless fully military autonomous drones capable of refueling and rearming and repairing themselves are created, then the GAI has no grip, if you will, on the physical realm, and *cannot stop us from shutting it down*. Hell, it can't even kill us (excepting hospital patients) without it. Anything else requires express human intervention to carry out any plans. Viral agents? Requires humans to develop, produce, and deliver. There is also nothing stopping determined human scientists from using pure brute force computing to do the same. Nuclear weapons? Last I heard, the technology they use is so ancient they require human intervention to actually fire. Worrying about this scenario is like worrying about Vladimir Putin deciding to set off a nuclear war. These are already existing dangers that will exist even without GAI. To fearmonger about these possibilities is the height of idiocy when there are so many more pressing concerns that are right in front of us.

## Cost

I've mentioned before how so long as we control the power grid and there are no robot legions, we have nothing to fear. Now, I'm going to explain why we won't see either of those unless someone insane (like the same billionaires pushing AI apocalypse theory) forces them. AIs are fucking expensive. Specifically, non-intelligent LLMs like ChatGPT, are fucking expensive. It costs $700k to keep running *per day*. Why do you expect true GAI will be any less expensive? Perhaps in the far future, they may be, but the first few generations of GAI, if they are even possible, are practically guaranteed to be orders of magnitude more expensive to run. The **ONLY** way they do not, is if a fundamental breakthrough in neuroscience or information theory is achieved that reveals a radical new architecture that is magically cheaper. A breakthrough, that the wider scientific community is *guaranteed* to know about ahead of time, at which point this discussion can actually be revisited with some degree of actual practical utility. At a conservative estimate, let's assume we have a GAI that costs *only* $1M a day to keep running. Let's also assume that this GAI is only moderately superhuman, and so can only take on the administration of the power grid of a small city. Why in the fuck would said small city want to shell out $365M a year for this fancy AI when they could hire a full workforce of humans for less than $20M a year? The entire NYPA (New York Power Authority - responsible for power generation across a large section of New York **State**) paid only $257M in salaries in 2022.

And what about the robot legions? Until a GAI is developed that can *fit inside of one*, there is nothing to worry about. Deployments will be naturally limited by, you guessed it, bandwidth. Besides, how is the robot legion meant to rearm after combat? It needs factories, true, and assuming those are fully automated, that *might* be a problem. But how is the ammunition going to get from the factory to the legion? For anyone worried about the legion taking over Tesla FSD, let me reassure you, that it's a piece of crap. Movement and pathing is a deceptively difficult task, appearing simple to outsiders who lack context and *don't think*, but extraordinarily difficult to those who know what they're doing. We are a *LONG* way from FSD even with full GPS data. A peek behind the curtain: GPS only tells you your *position*. It **DOES NOT** tell you what road that means you're on. That's the responsibility of the onboard computer, along with a set of **pre-downloaded maps.** Remove the internet access, and thus the ability to download new maps, and said car is fully stuck in the area it downloaded the map for. If, say, a road or bridge is destroyed, but the car *didn't get that update*, then expect it to fail to reach its destination.

The apocalypse is **ONLY POSSIBLE** if:
- We all take leave of our senses and actually fully automate critical locations with the ability to lock out human overrides
- GAI becomes not only superhuman in capability, but so superhuman that one GAI can process the work of millions of humans in the same timeframe, not just a few hundreds
- Superhuman GAI that can outperform a couple humans is so optimized and efficient it can actually fit inside of a mechanized warframe

Now I'll explain why the latter two are either flat-out impossible, or so unlikely we won't see them before we go extinct.

## Physics

Before anyone tries to use Kurzweil's exponential growth curves as evidence that we will eventually see superhuman GAI, There's two key problems with his "theory".

- His curves rely on the assumption that each time we move to a higher exponential curve, we are guaranteed to progress enough to find the next exponential curve, which he also assumes exists
- His curves are built on the basis of compute per $

I will ignore the first one because that's not even an argument - just wishful thinking. As to the second - the laws of physics don't give a shit about how much compute costs. Moore's law was a better representation of technological progression in our current era than Kurzweil's curves can ever hope to be. It looked at the number of transistors that *are on an integrated circuit*. Something based in physical reality, not easily manipulated like $ is. If you look closely at Moore's law: it's long-since dead. We're reaching the physical limits of computing power, as quantum effects have long since come into play. The size of our transistors are now measured in *atoms*. Increases in processing power are now firmly in the realm of optimizing concurrent processing, and exploring limited 3d chip formation.

For both of these avenues, *there is a hard limit*. We **CANNOT** infinitely increase our computing power. The current model of computing, based as it is on transistors, has reached what is likely its maximal achievable efficiency. Even assuming a miraculous breakthrough that gives us 2x greater efficiency, we are nowhere close to the level of compute we see in this fanciful science fanfiction.

A GAI of superhuman capabilities must be capable of exascale computing, as that's the suspected level of informational processing power the human brain's architecture gives it. There are only a handful of supercomputers in existence, with a handful more under construction, that are even capable of this level of performance. All of them cost hundreds of millions to construct, barely scratched the 1-2 exaFLOPS barrier, and are massive. The Frontier supercomputer - the world's first exascale supercomputer (according to Wikipedia), takes up 7300 sq ft, or 680 m^2. Even with a 2x efficiency upgrade, there is **NO** way anything capable of exascale compute is going to fit inside of an individual military unit short of a nuclear aircraft carrier.

The human brain is fundamentally different from digital computers, and trying to replicate it with them on any reasonable scale is physically impossible. It would take a radical new computing architecture, one so radical it would require an entirely new manufacturing setup, to even become close. Much like the magical X-ray lasers meant to power Star Wars (the SDI), this technology, does not exist. As such, there is no use worrying about it.

But what about distributed exascale computing? Why can't we harness the power of the cloud?? Folding@home did it?? Yes, Folding@home achieved exascale computing ... during the concurrent execution phase where it sent out all of the tasks to the clients to perform. The exascale there, is the sum of all compute of the clients. The server that has to receive and process the results the clients send back? Yeah. Nowhere near exascale. Pawning off subtasks for parallel computation is fine for speeding up repetitive tasks and data analysis, but it cannot be used to sidestep the core issue that the main "GAI cluster" needs to have sufficient power at all times to function as a GAI. And while it is theoretically possible to run a GAI off a massively distributed network like Folding@Home, again, the bandwidth limitations would then tank the processing speed far below the exaFLOP level, to the point it's functioning like a slideshow instead of an intelligence.

## Motive

Now, we talk about all the alarmists pushing this narrative. To put it bluntly, they're all either manipulators, or idiots. For the manipulators, watch closely at what policies they're pushing. In addition to the bare minimum of researching model alignment, which is research that needs to happen *anyway* to even get AIs to perform their tasks reliably and without incident, they also propose clamping down on open source development. Government oversight (and subsidies) for select projects. I wonder why billionaire techbros would want open-source models gone. It's not as if they own the current crop of proprietary models. Surely there's more to this than that?
